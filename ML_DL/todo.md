## TOdo 

### Basic
* Overfitting vs Underfitting
* Bias/Variance trade off
* How to avoid overfitting
* Generative vs Discrimitive

### Reguarlization
* L1 vs L2 (difference, Lasso/Ridge)
* why choose on over the other
* Why regularization works

### [Metric](MLDocs/Metrics.md)

## Loss and Optimizer
* MSE
* Linear Regression 最小二乘法 and MSE
* relative entropy, cross entropy
* K-L divergence
* loss of logistric regression
* loss of SVM
* when and why use cross entropy
* Decision tree split node



## Deep Learning

### To Learn
- Activation: swish

Recall the other activation functions if possible.


## To Recall


- Batch Normlization

    momentum

- Dropout

    drop_rate, noise_shape

- ZeroPadding
