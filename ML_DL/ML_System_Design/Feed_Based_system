#  Twitter feed system
https://www.educative.io/courses/grokking-the-machine-learning-interview/B1X08zyx05N

## Features

1. The logged-in user
2. The Tweet
3. The author of the tweet
4. The context

### User-author Features

* User-author historical interactions: 

e.g. author_liked_posts_3months, author_liked_posts_1year

* User-author similarity: 

e.g. common_followees, topic_similarity, tweet_content_embedding_similarity, social_embedding_similarity


### Author Features

* Author's degree of influence: 

e.g. is_verified, author_social_rank, author_num_followers, follower_to_following_ratio

* Historical trend of interactions on the author's Tweets: 
e.g. author_engagement_rate_3months, author_topic_engagement_3months

### User-Tweet Features

* Features based on Tweet's content: 
e.g. Tweet_length, Tweet_recency, is_image_video, is_URL

* Features based on Tweet's interaction: 
e.g.num_total_interactions (time decay model, or different time window), interactions_in_last_1_hour, interactions_in_last_1_day, interactions_in_last_3_days, interactions_in_last_week

* Separate features for different engagements

### Context-based Features

e.g. day_of_week, time_of_day, current_user_location, season, lastest_k_tag_interactions, approaching_holiday

### Sparse features

e.g: tweets_id, user_id, unigrams/bigrams of a Tweet


## Training Data Generation

### Balance positive and negative training examples

Assume 100 million Tweets are viewed collectively by the users in a day, with 5% engagement ratio, thus 5 million positive examples vs 95 million negative examples. 

Assume 10 million examples are used, balance the ratio of positive and negative training examples, randomly downsample:

* negative examples to 5 million samples
* positive examples to 5 million samples

### Train test split

1st, 2nd week as training, 3rd week as validation.


## Model Design (Ranking)

### Model Options

#### Logistic Regression

* Pros: 

A key advantage of using logistic regression is that it is reasonably fast to train. This enables you to test new features fairly quickly to see if they make an impact on the AUC or validation error. Also, it's extremely easy to understand the model. You can see from the feature weights which features have turned out to be more important than others.

* Cons

A major limitation of the linear model is that it assumes linearity exists between the input features and predictin. Therefore, you have to manually model feature interactions. 

(For example, if you believe that the day of the week before a holiday will have a major import on your engagement prediciton, you will have to create this feature manually. Other like tree-based and neural networks are able to learn these feature interactions.)

Regularization 

### MART (Multiple additive regression tree)

Additive tree-based models, e.g., boosted decision trees and random forest. 

Trees are inherently able to utilize non-linear relations between features. Tree-based models also don't requre a large amount of data as they are able to generalize well quickly. So a few million examples should be good enough to give us an optimal model.

Hyperparemeters to play around:

* Number of trees
* Maximum depth
* Minimum samples needed for split
* Maximum features sampled for a split


### Deep Learning

Can use multi-layer approach as deep learning approach can be computational expensive. Simpler model for stage 1 and a complex for stage 2.

Hyperparemeters to play around:

* Learning Rate
* Number of hidden layers
* Batch Size
* Number of epochs
* Dropout rate for regularizing model and avoiding overfitting

Separate neural networks

Multi-task neural networks: Shared layers + task specific specialized layers (like, comment, or retweet)


### Online Learning

tree-based models + neural networks to generate features and use a linear model 


### Diversity

* Why

Users would eventually get bored of seeing Tweets from the same author repeatedly. Also for the similar content/image/video type of the Tweet.

* Introducing the repetition penalty

1. Add a negative weight to the Tweet's score upon repetition

2. Bring the Tweet with repetition three steps down in the sorted list.


## 5. Model Evaluation

### Offline evaluation

* Evaluated on unseen data

    * k-fold CV, 
    * 1st, 2nd week as training, 3rd week as validation.

* AUC, F1 Score

### Online A/B testing Evaluation

* 1% of the 500 million active users => 5 million active users for the A/B test.

* Two buckets, each having 2.5 million users.

* Bucket one users : control group shown timelines according to the time-based model; Bucket two users: shown timelines according to the new ranking model.

* Retrain on latest data

### To deploy or not to deploy

Observe the engagement increasement percentage, use a statistical significance (like p-value) to ensure that the gain is real

Also consider the complexity especially for smaller gains.


